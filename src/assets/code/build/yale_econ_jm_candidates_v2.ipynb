{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE WORKS FOR COHORTS FROM 2019-2022\n",
    "# Website of job market candidates that we want to scrape\n",
    "# Note the year, as it will be used to save the CSV output later:\n",
    "# 2017\n",
    "url = \"https://web.archive.org/web/20170302034545/https://economics.yale.edu/graduate/graduate-placement\"\n",
    "\n",
    "# Get HTML content of the website\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "# Initialize lists\n",
    "names = []\n",
    "cv_links = []\n",
    "personal_websites = []\n",
    "dissertations = []\n",
    "fields_of_interest = []\n",
    "committee_members = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAMES\n",
    "headers = soup.find_all(\"h3\")\n",
    "for header in headers:\n",
    "    name = header.get_text(strip=True)\n",
    "    names.append(name)\n",
    "\n",
    "# CV LINKS & PERSONAL WEBSITES\n",
    "\n",
    "# Find all links\n",
    "atags = soup.find_all('a')\n",
    "atags\n",
    "\n",
    "for atag in atags:\n",
    "    if 'CV' in atag.get_text():\n",
    "        # Extract link from the tag\n",
    "        cv_link = atag.get(\"href\")\n",
    "        # Append the link to the list\n",
    "        cv_links.append(cv_link)\n",
    "    \n",
    "    if 'Website' in atag.get_text():\n",
    "        # Extract link from the tag\n",
    "        personal_website = atag.get('href')\n",
    "        # Append the link to the list\n",
    "        personal_websites.append(personal_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary for uniqueness and order\n",
    "dissertations_dict = {}\n",
    "\n",
    "# Find all <strong> tags with any sibling text\n",
    "strongtags = soup.find_all('strong')\n",
    "\n",
    "for strongtag in strongtags:\n",
    "    text = strongtag.get_text(strip=True) # Clean whitespace\n",
    "    # DISSERTATION TITLES\n",
    "    if 'Dissertation Title' in text:\n",
    "        dissertation_title = strongtag.next_sibling.strip()\n",
    "        dissertations_dict[dissertation_title] = None\n",
    "    elif 'Job Market Paper' in text:\n",
    "        jm_paper = strongtag.next_sibling.strip()\n",
    "        dissertations_dict[jm_paper] = None\n",
    "\n",
    "# Convert to list while preserving order\n",
    "dissertations = list(dissertations_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m sibling \u001b[38;5;241m=\u001b[39m area\u001b[38;5;241m.\u001b[39mnext_sibling\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Skip empty or invalid siblings\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m sibling \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msibling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:  \n\u001b[1;32m     10\u001b[0m     sibling \u001b[38;5;241m=\u001b[39m sibling\u001b[38;5;241m.\u001b[39mnext_sibling\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add to list if valid\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "areas = soup.find_all('strong', string=lambda text: text and 'Areas of Concentration' in text)\n",
    "\n",
    "for area in areas:\n",
    "    text = area.get_text(strip=True)  # Clean whitespace\n",
    "    if 'Areas of Concentration:' in text:\n",
    "        # Safely navigate siblings\n",
    "        sibling = area.next_sibling\n",
    "        # Skip empty or invalid siblings\n",
    "        while sibling and not sibling.strip():  \n",
    "            sibling = sibling.next_sibling\n",
    "        # Add to list if valid\n",
    "        if sibling:\n",
    "            field = sibling.strip()\n",
    "            fields_of_interest.append(field)\n",
    "\n",
    "# Print the results to verify\n",
    "fields_of_interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# areas = soup.find_all('strong', string=lambda text: text and 'Areas of Concentration' in text)\n",
    "\n",
    "# for area in areas:\n",
    "#     text = area.get_text(strip=True) # Clean whitespace\n",
    "#     # AREAS OF CONCENTRATION\n",
    "#     if 'Areas of Concentration:' in text:\n",
    "#         field = area.next_sibling.strip()\n",
    "#         fields_of_interest.append(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a dictionary for uniqueness and order\n",
    "# dissertations_dict = {}\n",
    "# fields_of_interest_dict = {} #WAIT! I don't want this to be a dictionary because there's repeat values!!\n",
    "\n",
    "# # Find all <strong> tags with any sibling text\n",
    "# strongtags = soup.find_all('strong')\n",
    "\n",
    "# for strongtag in strongtags:\n",
    "#     text = strongtag.get_text(strip=True) # Clean whitespace\n",
    "#     # DISSERTATION TITLES\n",
    "#     if 'Dissertation Title' in text:\n",
    "#         dissertation_title = strongtag.next_sibling.strip()\n",
    "#         dissertations_dict[dissertation_title] = None\n",
    "#     elif 'Job Market Paper' in text:\n",
    "#         jm_paper = strongtag.next_sibling.strip()\n",
    "#         dissertations_dict[jm_paper] = None\n",
    "    \n",
    "#         # FIELDS OF INTEREST\n",
    "#     if 'Areas of Concentration:' in text:\n",
    "#         field = strongtag.next_sibling.get_text(strip=True)\n",
    "#         fields_of_interest_dict[field] = None\n",
    "\n",
    "# # Convert to list while preserving order\n",
    "# dissertations = list(dissertations_dict.keys())\n",
    "# fields_of_interest = list(fields_of_interest_dict.keys())\n",
    "\n",
    "# fields_of_interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIELDS OF INTEREST, DISSERTATIONS\n",
    "\n",
    "# # Create a set to store unique values temporarily\n",
    "# dissertations_set = set() # set() removes duplicate values\n",
    "# fields_of_interest_set = set()\n",
    "\n",
    "# # Find all <strong> tags with any sibling text\n",
    "# strongtags = soup.find_all('strong')\n",
    "\n",
    "# for strongtag in strongtags:\n",
    "#     text = strongtag.get_text(strip=True)  # Clean whitespace\n",
    "#     # DISSERTATION TITLES\n",
    "#     if 'Dissertation Title' in text:\n",
    "#         dissertation = strongtag.next_sibling.strip()\n",
    "#         dissertations_set.add(dissertation)  # Add to set for uniqueness\n",
    "#     elif 'Job Market Paper' in text:\n",
    "#         dissertation = strongtag.next_sibling.strip()\n",
    "#         dissertations_set.add(dissertation)  # Add to set for uniqueness\n",
    "#     # FIELDS OF INTEREST\n",
    "#     elif 'Areas of Concentration' in text:\n",
    "#         field = strongtag.next_sibling.strip()\n",
    "#         fields_of_interest_set.add(field)\n",
    "        \n",
    "\n",
    "# # Convert the set back to a list\n",
    "# dissertations = list(dissertations_set)\n",
    "# fields_of_interest = list(fields_of_interest_set)\n",
    "\n",
    "# len(dissertations)\n",
    "\n",
    "# # print(len(dissertations),\n",
    "# # len(fields_of_interest),\n",
    "# # len(headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 9 9 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"field-content\">Yaniv Ben-Ami</h3>,\n",
       " <h3 class=\"field-content\">Gregory Cox</h3>,\n",
       " <h3 class=\"field-content\">Sharat Ganapati</h3>,\n",
       " <h3 class=\"field-content\">Matthew Grant</h3>,\n",
       " <h3 class=\"field-content\">Chiara Margaria</h3>,\n",
       " <h3 class=\"field-content\">Craig Palsson</h3>,\n",
       " <h3 class=\"field-content\">Meredith Startz</h3>,\n",
       " <h3 class=\"field-content\">Haobin Wang</h3>,\n",
       " <h3 class=\"field-content\">Jeffrey Weaver</h3>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUGGING\n",
    "\n",
    "# Find the maximum length of the lists\n",
    "max_length = max(len(names), len(dissertations), len(fields_of_interest), len(cv_links), len(personal_websites))\n",
    "\n",
    "# Pad the lists with None if they are shorter than max_length\n",
    "names += [None] * (max_length - len(names))\n",
    "dissertations += [None] * (max_length - len(dissertations))\n",
    "fields_of_interest += [None] * (max_length - len(fields_of_interest))\n",
    "cv_links += [None] * (max_length - len(cv_links))\n",
    "personal_websites += [None] * (max_length - len(personal_websites))\n",
    "\n",
    "print(len(names),\n",
    "    len(dissertations),\n",
    "    len(fields_of_interest),\n",
    "    len(cv_links),\n",
    "    len(personal_websites))\n",
    "\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>school</th>\n",
       "      <th>year</th>\n",
       "      <th>cv_link</th>\n",
       "      <th>personal_website</th>\n",
       "      <th>dissertation</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yaniv Ben-Ami</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gregory Cox</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sharat Ganapati</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matthew Grant</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chiara Margaria</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Craig Palsson</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Meredith Startz</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Haobin Wang</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jeffrey Weaver</td>\n",
       "      <td>Yale</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://web.archive.org/web/20170302034545/htt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name school  year  \\\n",
       "0    Yaniv Ben-Ami   Yale  2017   \n",
       "1      Gregory Cox   Yale  2017   \n",
       "2  Sharat Ganapati   Yale  2017   \n",
       "3    Matthew Grant   Yale  2017   \n",
       "4  Chiara Margaria   Yale  2017   \n",
       "5    Craig Palsson   Yale  2017   \n",
       "6  Meredith Startz   Yale  2017   \n",
       "7      Haobin Wang   Yale  2017   \n",
       "8   Jeffrey Weaver   Yale  2017   \n",
       "\n",
       "                                             cv_link personal_website  \\\n",
       "0  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "1  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "2  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "3  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "4  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "5  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "6  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "7  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "8  https://web.archive.org/web/20170302034545/htt...             None   \n",
       "\n",
       "  dissertation field  \n",
       "0               None  \n",
       "1         None  None  \n",
       "2         None  None  \n",
       "3         None  None  \n",
       "4         None  None  \n",
       "5         None  None  \n",
       "6         None  None  \n",
       "7         None  None  \n",
       "8         None  None  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"name\": names,\n",
    "    \"school\": 'Yale',\n",
    "    \"year\": '2017',\n",
    "    'cv_link': cv_links,\n",
    "    'personal_website': personal_websites,\n",
    "    'dissertation': dissertations,\n",
    "    'field': fields_of_interest,\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with empty cells:\n",
      "1\n",
      "          name school  year cv_link personal_website  \\\n",
      "19  Ling Zhong   Yale  2019    None             None   \n",
      "\n",
      "                         dissertation  \\\n",
      "19  \"Three Essays in Labor Economics\"   \n",
      "\n",
      "                                                field  \n",
      "19  Labor Economics, Economics of Education, Publi...  \n"
     ]
    }
   ],
   "source": [
    "# Identify and flag empty cells\n",
    "empty_cells = df.isnull()\n",
    "empty_rows = df[empty_cells.any(axis=1)]\n",
    "\n",
    "print(\"\\nRows with empty cells:\")\n",
    "print(len(empty_rows)) # Length should be zero, but it's 6\n",
    "print(empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               name school  year  \\\n",
      "0     Shameel Ahmad   Yale  2019   \n",
      "1    Antoine Arnoud   Yale  2019   \n",
      "2     Leila Bengali   Yale  2019   \n",
      "3  Anna Bykhovskaya   Yale  2019   \n",
      "4    Jin-Wook Chang   Yale  2019   \n",
      "\n",
      "                                             cv_link  \\\n",
      "0  https://web.archive.org/web/20190329023737/htt...   \n",
      "1  https://web.archive.org/web/20190329023737/htt...   \n",
      "2  https://web.archive.org/web/20190329023737/htt...   \n",
      "3  https://web.archive.org/web/20190329023737/htt...   \n",
      "4  https://web.archive.org/web/20190329023737/htt...   \n",
      "\n",
      "                                    personal_website  \\\n",
      "0  https://web.archive.org/web/20190329023737/htt...   \n",
      "1  https://web.archive.org/web/20190329023737/htt...   \n",
      "2  https://web.archive.org/web/20190329023737/htt...   \n",
      "3  https://web.archive.org/web/20190329023737/htt...   \n",
      "4  https://web.archive.org/web/20190329023737/htt...   \n",
      "\n",
      "                                        dissertation  \\\n",
      "0     \"Demography and Development in Colonial India\"   \n",
      "1  \"Essays in Macroeconomics and Computational Ec...   \n",
      "2  \"Essays on Information Use in Consumer Decisio...   \n",
      "3            \"Peer Effects:  Theory and Measurement\"   \n",
      "4         \"Essays on Debt, Collateral, and Networks\"   \n",
      "\n",
      "                                               field  \n",
      "0  Economic History, Development Economics, Inter...  \n",
      "1            Macroeconomics, Computational Economics  \n",
      "2  Behavioral Economics, Public Finance, Applied ...  \n",
      "3  Time Series Econometrics, Matching and Market ...  \n",
      "4  Financial Economics, Macroeconomics, Microecon...  \n"
     ]
    }
   ],
   "source": [
    "# Save the data to a CSV file, making sure to specify the year\n",
    "df.to_csv(\"yale_econ_jm_candidates_2017.csv\", index=False)\n",
    "\n",
    "#Check first few columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING 2021-- KEEP WORKING ON THIS\n",
    "# Manually drop duplicate CV link for Lopez-Pena\n",
    "# df = df.drop(df.loc[df['cv_link'] == 'https://web.archive.org/web/20210228201841/https://www.dropbox.com/s/c5awtuhrdvno4al/cv_lopezpena_201102.pdf?dl=0'].index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
