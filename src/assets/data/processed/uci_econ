from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse
import pandas as pd

# Website of job market candidates that we want to scrape
url = "https://www.economics.uci.edu/grad/jobmarket.php"

# Get the HTML text from the website
page = requests.get(url)
soup = BeautifulSoup(page.text, 'html.parser')

def get_urls_by_text(soup, text):
    unique_urls = []
    # Find all elements with class "one-third"
    one_third_elements = soup.find_all('div', class_='one-third')
    
    for element in one_third_elements:
        # Check if the element contains the specified text
        if text in element.get_text():
            for link in element.find_all('a', href=True):
                href = link.get('href')
                if href and href not in unique_urls:
                    unique_urls.append(href)
    return unique_urls

def get_job_market_paper_links(soup, text):
    unique_urls = []
    # Find all elements with class "one" that contain the specified text
    one_elements_with_text = soup.find_all(lambda tag: tag.name == 'div' 
                                            and 'one' in tag.get('class', []) 
                                            and text in tag.get_text() 
                                            and ("title" not in tag.attrs 
                                                 or ("title" in tag.attrs 
                                                     and "CV" not in tag["title"] 
                                                     and "Website" not in tag["title"])))

    for element in one_elements_with_text:
        for link in element.find_all('p', title=True):
            title = link.get('title')
            if title and "CV" not in title and "Website" not in title:
                href = link.get('href')
                if href and href not in unique_urls:
                    unique_urls.append(href)
    return unique_urls


# Get the base URL
base_url = "{0.scheme}://{0.netloc}".format(urlparse(url))

# Extract URLs for CVs and Websites from the main page and remove duplicates
cv_link = get_urls_by_text(soup, "CV:")
personal_website = get_urls_by_text(soup, "Website:")

# Extract job market paper links
job_market_paper_links = get_job_market_paper_links(soup, "Job Market Paper:")

# Create a pandas DataFrame with separate columns for CV links, Website links, and Job Market Paper links
df = pd.DataFrame({
    'CV Link': pd.Series(cv_link),
    'Personal Website': pd.Series(personal_website),
    'Job Market Paper Link': pd.Series(job_market_paper_links)
})

# Print the DataFrame
print(df)

# Save the DataFrame to a CSV file
df.to_csv('job_market_candidates.csv', index=False)
